\section{Conclusion and future work}
\label{fut}
\label{conc}
% - \gls{tosca} blueprints are conceptual X
% - The VM and Kubernetes was deployed manually X
% - Full implementation developed needed with an orchestrator such as e.g. \gls{drip} X
% - \gls{ndn} is still experimental X
% performance tweaks that require source code changes not implemented, up for upstream.

% add the part about caching strategies and why we used the general recommendation and not the one for big data
% Initially, we started researching the state of the art technologies for creating our design. We came to the conclusion that \gls{ndn} would fit the technical challenges we had to work out.
% After, we started researching \gls{pid} interoperability with the \gls{ndn} namespace and found out that our principles can be adhered as discussed in section \ref{pid-poc}.



% You can explain that a detailed analysis on the \gls{ndn} caching strategies and data infrastructure sharing patterns need future analysis. This is yet investigated by the thesis. You can leave it as a future work.


% NEW AND FINAL FEEDBACK - KEES
% highlight the merging of the 2 projects in conclusion

% highlight that the method needs to be tested in large scale deployment to evaluate it in practice





Our research investigated solutions for the increasing trend of data producers and consumers in research clouds. Our methods are designed to be used for large research clouds. However, the methods were only tested in a small scale proof of concept. Therefore, further experimentation is needed on a larger scale to fully prove our methods. Furthermore, data producers and consumers can make use of different \gls{pid} standards, in order to retrieve this data by name in NDN, a compatible translation was needed between these namespaces. The solution we developed for this problem is designed to be easily extensible for new \gls{pid} types. 

Research clouds are in general a federated cloud, where each federation is responsible for its own budget and infrastructure. However, our solution assumes central control over the NDN. Our solution could be approached in several ways. The solutions could be deployed as an internal data-sharing platform per infrastructure and interconnect those NDNs, thus maintaining the federated model. Or, it could be deployed as a third party data-sharing platform, where one can deploy and operate the \gls{ndn} for multiple infrastructures. Our research did not explore these subjects. However, it provides the technical possibilities to deploy these solutions in such a manner.

% ANAS - SOME CONCLUSIONS SUMMERIZED ABOUT THE PEFORMANCE MEASUREMENTS - IS \gls{ndn} PROMISING OR NOT? FURTHER RESEARCH IS DISCUSSED BELOW

% We have developed a method of deployment to make \gls{ndn} scalable from a single infrastructure description. This was done by utilizing the properties of \gls{tosca} templates, cloud and virtualization techniques. Furthermore, data producers and consumers can make use of different \gls{pid} standards, in order to retrieve this data by name in NDN, a compatible translation was needed between these namespaces. The solution we developed for this problem is designed to be easily extensible for new \gls{pid} types.

% presented a method and design for sharing digital objects using \gls{ndn} with support for multiple \gls{pid} providers. Our findings give insight for distributed infrastructures that manage large and diverse sets of data such as SeaDataCloud and CS3.

Based on our findings we acknowledge the following subjects for further research. As described in section \ref{overview-tosca}, \gls{tosca} orchestrators are still in development. Therefore, more experimentation is needed when these orchestrators become more mature. This can give more insight in the flexibility and utilization of these orchestrators. As discussed in section \ref{planning-poc}, due to the nature of Kubernetes, pods are deployed based on the resource requirements of a pod and the resource availability inside a cluster. Pods that run \gls{ndn} functions are not only interested in Kubernetes resources, but their true value is mainly determined on locality. This is due to the distributed nature of NDN, which depends on in-network caching along the network path between data producers and consumers. If Kubernetes doesn't deploy a pod where \gls{ndn} resources are needed, then human intervention is needed to specify on which Kubernetes node a pod must run. Therefore, if Kubernetes could be extended with the intelligence of also NDN's resource needs, it could deploy pods automatically in a certain geographical area in order to increase cache hits. Furthermore, the \gls{ndn} faces were configured manually as well, this was due to the lack of mature \gls{ndn} routing protocols. However there are two promising routing protocols in development; \gls{ospfn} by Lan Wang et. al. \cite{ndn-ospfn1, ndn-ospfn2} and \gls{nlsr} \cite{nlsr}. With a routing protocol, the \gls{ndn} management process would become less complicated and more resilient.

Some \gls{ndn} performance issues were identified in related work, such as when using UDP and the use of long names - mention UDT as future work for integration with \gls{ndn} since UDP in \gls{ndn} does not utilize congestion and flow control mechanisms.

Furthermore, metadata can be parsed to fill in possible naming gaps in an \gls{ndn} name. An \gls{ndn} name is unbounded, however, the length of \gls{ndn} names have to be taken into account since long \gls{ndn} names can degrade performance (as discussed in section \ref{introduction-related-work-ndn}). So et al. researched and developed a solution for fixed lookup times. However, this research was not made public. Integrating such a solution could prove valuable for \gls{ndn} performance. Lastly, we discussed preliminary performance measurements based on a best-effort testing scenario. Our results show that \gls{ndn} over TCP gives the best performance in comparison to \gls{ndn} over UDP, which correlates with related work.

Furthermore, more performance measurements and analyses is needed to gather a better profile of \gls{ndn} scalability with cloud resources. The performance results we discussed in section \ref{disc} are preliminary. We recommend the following test scenarios. Performance can be improved with the use of different MTU sizes (this can be configured when creating a face, as well as with the \texttt{ndnputchunks} tool) and configuring congestion threshold and pipeline types (for example fixed, Cubic or AIMD for configuring window sizes). Where the latter can also be configured when using the \texttt{ndnputchunks} tool. Furthermore, the \gls{ndn} can be configured to use TCP or UDP, which can also be configured when creating a face. We recommend running different tests with the aforementioned parameters with multiple consumers in order to determine when congestion occurs. The parameters can also be combined with the most optimal \gls{ndn} caching mechanism and the most optimal ordering method \cite{koulouzis2018information} to run benchmarks. These performance tests would be interesting by running it in a cloud environment, where resources can be scaled up and down.

% This part needs some tweaking maybe (already tweaked it a bit)
%For benchmarking, the tool ndn-traffic-generator\footnote{https://github.com/named-data/ndn-traffic-generator} can be used to generate random interest and data traffic in an \gls{ndn} network. Although this tool only allows a fixed pipeline size (\texttt{X} amount of packets per second) and does not implement congestion control. Thus it is not very useful when comparing against TCP/IP \cite{ndnput-mem}. The tool \texttt{ndncatchunks} that we used in our proof of concept for requesting objects from NDN, is a more realistic scenario than generating random traffic as this does incorporates congestion control, which makes it more useful for comparing against TCP/IP.

The \gls{pid} interoperability solution we developed exists outside of the \gls{ndn} source code. For the best application of this functionality, we recommend the integration of these interoperability functionalities into the native \gls{ndn} source code. This would remove the need to run a translation gateway.

% Implementing routing protocols for \gls{ndn} in our proof of concept is also worth testing. As in  our proof of concept, we manually configure all the routes.

% Lan Wang et. al. \cite{ndn-ospfn1} designed and performed a feasibility assessment in 2011 of an OSPF extension for \gls{ndn} that they call "OSPF-N". OSPF-N is a routing protocol based on the existing OSPF protocol that operates on the network layer for IP networks. OSPF-N uses new types of opaque LSA's to carry \gls{ndn} routing information. By using the OSPF-N configuration manager at each \gls{ndn} node, it configures links to other \gls{ndn} nodes, name prefixes of the content store, and the maximum number of hops to be calculated for each name prefix in order to support multipath routing \cite{ndn-ospfn2}. \gls{nlsr} is also a routing protocol designed for NDN, in addition to OSPF-N. The initial design for \gls{nlsr} was developed in 2013. The protocol operates at the application layer similar to existing IP routing protocols such as RIP and BGP. This protocol calculates the routing table using link-state or hyperbolic routing. Furthermore, it produces multiple faces for each reachable name prefix in a single authoritative domain .
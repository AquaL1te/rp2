\section{Conclusion and future work}
\label{fut}
\label{conc}
% - TOSCA blueprints are conceptual X
% - The VM and Kubernetes was deployed manually X
% - Full implementation developed needed with an orchestrator such as e.g. DRIP X
% - NDN is still experimental X
% performance tweaks that require source code changes not implemented, up for upstream.

% add the part about caching strategies and why we used the general recommendation and not the one for big data
% Initially, we started researching the state of the art technologies for creating our design. We came to the conclusion that NDN would fit the technical challenges we had to work out.
% After, we started researching PID interoperability with the NDN namespace and found out that our principles can be adhered as discussed in section \ref{pid-poc}.



% You can explain that a detailed analysis on the NDN caching strategies and data infrastructure sharing patterns need future analysis. This is yet investigated by the thesis. You can leave it as a future work.




In this research paper we investigated solutions for the increasing trend of data producers and consumers in research clouds. NDN is an experimental solution to distribute data by using in-network caching. We have developed a method of deployment to make NDN scalable from a single infrastructure description. This was done by utilizing the properties of TOSCA templates, cloud and virtualization techniques. Furthermore, data producers and consumers can make use of different PID standards, in order to retrieve this data by name in NDN, a compatible translation was needed between these namespaces. The solution we developed for this problem is designed to be easily extensible for new PID types. Furthermore, metadata can be parsed to fill in possible naming gaps in an NDN name. An NDN name is unbounded, however, the length of NDN names have to be taken into account since long NDN names can degrade performance. Lastly, we discussed preliminary performance measurements based on a best-effort testing scenario. Our results show that NDN over TCP gives the best performance, which correlates with related work.

% presented a method and design for sharing digital objects using NDN with support for multiple PID providers. Our findings give insight for distributed infrastructures that manage large and diverse sets of data such as SeaDataCloud and CS3.
Based on our findings we acknowledge the following subjects for further research. As described in section \ref{overview-tosca}, TOSCA orchestrators are still in development. Therefore, more experimentation is needed when these orchestrators become more mature. This can give more insight in the flexibility and utilization of these orchestrators. As discussed in section \ref{planning-poc}, due to the nature of Kubernetes, pods are deployed based on the resource requirements of a pod and the resource availability inside a cluster. Pods that run NDN functions are not only interested in Kubernetes resources, but their true value is mainly determined on locality. This is due to the distributed nature of NDN, which depends on in-network caching along the network path between data producers and consumers. If Kubernetes doesn't deploy a pod where NDN resources are needed, then human intervention is needed to specify on which Kubernetes node a pod must run. Therefore, if Kubernetes could be extended with the intelligence of also NDN's resource needs, it could deploy pods automatically in a certain geographical area in order to increase cache hits. Furthermore, the NDN faces were configured manually as well, this was due to the lack of mature NDN routing protocols. However there are two promising routing protocols in development; \gls{ospfn} by Lan Wang et. al. \cite{ndn-ospfn1, ndn-ospfn2} and \gls{nlsr} \cite{nlsr}. With a routing protocol, the NDN management process would become less complicated and more resilient.

Furthermore, more performance measurements and analyses is needed to gather a better profile of NDN scalability with cloud resources. The performance results we discussed in section \ref{disc} are preliminary. We recommend the following test scenarios. Performance can be improved with the use of different MTU sizes (this can be configured when creating a face, as well as with the \texttt{ndnputchunks} tool) and configuring congestion threshold and pipeline types (for example fixed, Cubic or AIMD for configuring window sizes). Where the latter can also be configured when using the \texttt{ndnputchunks} tool. Furthermore, the NDN can be configured to use TCP or UDP, which can also be configured when creating a face. We recommend running different tests with the aforementioned parameters with multiple consumers in order to determine when congestion occurs. The parameters can also be combined with the most optimal NDN caching mechanism and the most optimal ordering method \cite{koulouzis2018information} to run benchmarks. These performance tests would be interesting by running it in a cloud environment, where resources can be scaled up and down.

% This part needs some tweaking maybe (already tweaked it a bit)
%For benchmarking, the tool ndn-traffic-generator\footnote{https://github.com/named-data/ndn-traffic-generator} can be used to generate random interest and data traffic in an NDN network. Although this tool only allows a fixed pipeline size (\texttt{X} amount of packets per second) and does not implement congestion control. Thus it is not very useful when comparing against TCP/IP \cite{ndnput-mem}. The tool \texttt{ndncatchunks} that we used in our proof of concept for requesting objects from NDN, is a more realistic scenario than generating random traffic as this does incorporates congestion control, which makes it more useful for comparing against TCP/IP.

The PID interoperability solution we developed exists outside of the NDN source code. For the best application of this functionality, we recommend the integration of these interoperability functionalities into the native NDN source code. This would remove the need to run a translation gateway.

% Implementing routing protocols for NDN in our proof of concept is also worth testing. As in  our proof of concept, we manually configure all the routes.

% Lan Wang et. al. \cite{ndn-ospfn1} designed and performed a feasibility assessment in 2011 of an OSPF extension for NDN that they call "OSPF-N". OSPF-N is a routing protocol based on the existing OSPF protocol that operates on the network layer for IP networks. OSPF-N uses new types of opaque LSA's to carry NDN routing information. By using the OSPF-N configuration manager at each NDN node, it configures links to other NDN nodes, name prefixes of the content store, and the maximum number of hops to be calculated for each name prefix in order to support multipath routing \cite{ndn-ospfn2}. NLSR is also a routing protocol designed for NDN, in addition to OSPF-N. The initial design for NLSR was developed in 2013. The protocol operates at the application layer similar to existing IP routing protocols such as RIP and BGP. This protocol calculates the routing table using link-state or hyperbolic routing. Furthermore, it produces multiple faces for each reachable name prefix in a single authoritative domain .
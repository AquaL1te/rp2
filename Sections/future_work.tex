\section{Future work}\label{fut}
% - TOSCA blueprints are conceptual X
% - The VM and Kubernetes was deployed manually X
% - Full implementation developed needed with an orchestrator such as e.g. DRIP X
% - NDN is still experimental X
% performance tweaks that require source code changes not implemented, up for upstream.

The TOSCA blueprints we made are conceptual at the moment and as such our proof of concept lacks a TOSCA orchestrator such as DRIP as discussed in section \ref{overview-tosca}. In our case, we use the YAML configuration of Kubernetes to act as the TOSCA template description while Kubernetes acts as the orchestrator. This means that in our proof of concept the deployment of the VM and Kubernetes happens manually, as the orchestrators mentioned in section \ref{overview-tosca} are still in a prototype phase. 
Extending Kubernetes with intelligence is required to automatically deploy NDN routers geographically, which is achieved by using containers.
%is an option for automatic deployment of the VM and Kubernertes.
It is also worth looking more into well defining TOSCA templates and using the DRIP orchestrator to demonstrate scaling in or out resources to other cloud providers, as we only demonstrated scaling in our out the NDN application with Kubernetes. 
%Explore performance bottlenecks (benchmarking)" 

%In our proof of concept we deploy the VM and Kubernertes manually, as the orchestrators mentioned in section \ref{overview-tosca} are still in a prototype phase. 

There is still some research to do in the field of benchmarking NDN. A lot of parameters can be configured to look for the most optimal configuration for NDN networking. Such as exploring different MTU sizes (this can be configured when creating a face, as well as with the \texttt{ndnputchunks} tool) and configuring congestion treshold and pipeline type (for example fixed, cubic or aimd for configuring window size). Where the latter can also be configured when using the \texttt{ndnputchunks} tool. Furthermore, the NDN network can be configured to use TCP or UDP, which can also configured when creating a face. We recommend running different tests with the aforementioned test with multiple consumers, to find out to what extent congestion occurs.

%and running different tests with multiple consumers. 

%The most optimal performance of different NDN strategies, such as decision, cache replacement and forward strategies have already been looked into by Mousa \
The most optimal NDN caching mechanisms, such as cache decision-, cache replacement- and cache forward strategies have already been looked into by Mousa \cite{ndn-app-aware} and Karakannas \cite{icn-bd}. The most optimal ordering method to be used with the caching mechanisms has also been looked into by Rahaf and Koulozis et al. \cite{koulouzis2018information}, which can be combined with the aforementioned parameters to run performance tests.
For benchmarking, the tool ndn-traffic-generator\footnote{https://github.com/named-data/ndn-traffic-generator} can be used to generate random Interest and Data traffic in an NDN network. Although this tool only allows a fixed pipeline size (\texttt{X} amount of packets per second) and does not implement congestion control. Thus it is not very useful when comparing against TCP \cite{ndnput-mem}. The tool \texttt{ndncatchunks} that we used in our proof of concept for requesting objects in NDN, is a more natural scenario than generating random traffic as this does incorporates congestion control, which makes it more useful for comparing against TCP.

As for PID interoperability within the NDN namespace, we recommend incorporating the PID to NDN translation into NDN software natively which makes it easier for having NDN support PIDs out-of-the box. It is also worth mentioning that the results of our proof of concept are preliminary, as not only NDN is still in experimental phase but also the orchestrators that fit in our use case like DRIP are still in a prototype phase.
%as it worth mentioning that NDN is still in experimental phase. etc.
 
 
Testing routing protocols for NDN is also an important aspect, as in our proof of concept we have to manually configure all the routes. Lixia Zhang et. al. designed and performed feasibility assessment of an extension of OSPF for NDN that they call "OSPF-N". OSPF-N is a routing protocol based on the existing OSPF protocol for IP networks. OSPF-N uses new types of Opaque LSA's to carry NDN routing information. By using the OSPF-N configuration manager at each NDN node, it configures links to other NDN nodes, name prefixes of the content store, and the maximum number of hops to be calculated for each name prefix in order to support multipath routing \cite{ndn-ospfn}.
 
%Testing routing protocols such as OSPF-N is also important, as in our proof of concept we manually configure all the routes. OSPF-N is a routing protocol based on the existing OSPF protocol for IP networks. With OSPF-N 
%Extent Kubernetes with intelligence X
%Where to deploy NDN routers (containers)? X
%Incorporate the PID -> NDN translation into NDN software natively X




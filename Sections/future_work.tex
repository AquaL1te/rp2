\section{Future work}\label{fut}
% - TOSCA blueprints are conceptual X
% - The VM and Kubernetes was deployed manually X
% - Full implementation developed needed with an orchestrator such as e.g. DRIP X
% - NDN is still experimental X
% performance tweaks that require source code changes not implemented, up for upstream.

% add the part about caching strategies and why we used the general recommendation and not the one for big data

The TOSCA blueprints we made are conceptual at the moment and as such our proof of concept lacks a TOSCA orchestrator such as DRIP as discussed in section \ref{overview-tosca}. In our case, we use the YAML configuration of Kubernetes to act as the TOSCA template description while Kubernetes acts as the orchestrator. This means that in our proof of concept the deployment of the VM and Kubernetes happens manually, as the orchestrators mentioned in section \ref{overview-tosca} are still in a prototype phase. 
Extending Kubernetes with intelligence is required to automatically deploy NDN routers geographically, which is achieved by using containers.

It is also worth researching how to well define TOSCA templates and using the DRIP orchestrator to demonstrate scaling in or out resources to other cloud providers, as we only demonstrated scaling in our out the NDN application with Kubernetes. 
%Explore performance bottlenecks (benchmarking)" 

We advice to do more research in the field of benchmarking NDN. The results we discussed in section \ref{disc} are preliminary. There are different kinds of parameters that can be configured to optimize performance. Such as exploring different MTU sizes (this can be configured when creating a face, as well as with the \texttt{ndnputchunks} tool) and configuring congestion treshold and pipeline types (for example fixed, Cubic or AIMD for configuring window sizes). Where the latter can also be configured when using the \texttt{ndnputchunks} tool. Furthermore, the NDN network can be configured to use TCP or UDP, which can also be configured when creating a face. We recommend running different tests with the aforementioned parameters with multiple consumers, to find out to what extent congestion occurs.
The parameters can also be combined with the most optimal NDN caching mechanism and the most optimal ordering method \cite{koulouzis2018information} to run benchmarks.

% This part needs some tweaking maybe (already tweaked it a bit)
For benchmarking, the tool ndn-traffic-generator\footnote{https://github.com/named-data/ndn-traffic-generator} can be used to generate random interest and data traffic in an NDN network. Although this tool only allows a fixed pipeline size (\texttt{X} amount of packets per second) and does not implement congestion control. Thus it is not very useful when comparing against TCP/IP \cite{ndnput-mem}. The tool \texttt{ndncatchunks} that we used in our proof of concept for requesting objects from NDN, is a more realistic scenario than generating random traffic as this does incorporates congestion control, which makes it more useful for comparing against TCP/IP.

As for PID interoperability with the NDN namespace, we recommend incorporating the PID to NDN translation into NDN software natively which makes it easier for having NDN support PIDs out-of-the box. It is also worth mentioning that the results of our proof of concept are preliminary, as not only NDN is still in experimental phase but also the mentioned orchestrators in section \ref{planning-ndn}.
 
Testing routing protocols for NDN is also an important aspect, as in our proof of concept we have to manually configure all the routes. Lan Wang et. al. \cite{ndn-ospfn1} designed and performed a feasibility assessment in 2011 of an extension of OSPF for NDN that they call "OSPF-N". OSPF-N is a routing protocol based on the existing OSPF protocol that operates on the network layer for IP networks. OSPF-N uses new types of Opaque LSA's to carry NDN routing information. By using the OSPF-N configuration manager at each NDN node, it configures links to other NDN nodes, name prefixes of the content store, and the maximum number of hops to be calculated for each name prefix in order to support multipath routing \cite{ndn-ospfn2}. 
NLSR is also a routing protocol designed for NDN, in addition to OSPF-N. 
The initial design for NLSR was developed in 2013. The protocol operates at the application layer similar to existing IP routing protocols such as RIP and BGP. This protocol calculates the routing table using link-state or hyperbolic routing. Furthermore, it produces multiple faces for each reachable name prefix in a single authoritative domain \cite{nlsr}. 






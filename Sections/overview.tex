\section{State of the art}
\label{tech-oview}
% most recent stage in the development of a product, incorporating the newest ideas and features
% add discussion about these tech in introduction or conclusion of this section
% Analyze the requirement for your research questions

In this section we provide an overview of the technologies used in our methodology sections \ref{pid-poc} and \ref{planning-ndn}.

\subsection{PID types and naming scheme}\label{pid-types}
Over the years, different kinds of PIDs have emerged. The most widely used PID types are Handle (first major PID type introduced in 1994), DOI, PURL, URN and ARK \cite{pid-oview, odin, hdl}. For our proof of concept described in section \ref{pid-poc}, we will describe Handle, DOI and URN. For a technical overview of the aforementioned PID types we refer to the research done by Karakannas \cite{icn-bd}. 

The most widely used PID types make use of the same hierarchical naming scheme, which starts with a PID type identifier such as \texttt{"urn"}, \texttt{"handle"} or \texttt{"doi"}, followed by some kind of delimiter. The PID type identifier is usually embedded in the URL of the PID resolver naming authority. Such as \texttt{"http://hdl.handle.net/"} for resolving Handle PIDs, \texttt{"https://doi.pangaea.de/"} for resolving DOI PIDs or \texttt{"http://resolver.kb.nl/resolve?urn="} for resolving URN PIDs, followed by the PID of the digital object. This means that PIDs are usually associated with a resolver via a URL \cite{ids, icn-bd}. A PID web resolver redirects to the location of the digital object. After the first delimiter, the naming authority is defined (this can be seen as a prefix of a digital object), followed again by some kind of delimiter. Finally there is a specific string for indicating the namespace, which is the identity of a digital object within the naming authority which syntax depends on the naming
authority. Table \ref{tab:pid} below shows an overview of the mentioned PID types. 
%For a more in-depth description of PIDs we refer to \cite{icn-bd} and \cite{pid-oview}.

\begin {table}[H]
\caption {Hierarchical scheme of PID standards \cite{icn-bd}.} \label{tab:pid} 
\begin{center}
\scalebox{0.82}{%
  \begin{tabular}{| c | c | c | c | c | c | }
    \hline
    \textbf{PID types} & \textbf{PID Type ID} & \textbf{Delimiter} & \textbf{Authority} & \textbf{Delimiter} & \textbf{Name} \\ \hline
    \textbf{URN} & urn & : & \textless NID\textgreater & : & \textless NSS\textgreater \\ \hline
    \textbf{Handle} & hdl & : & \textless Handle Naming Authority\textgreater & / & \textless Handle Local Name\textgreater \\ \hline
    \textbf{DOI} & doi & : & 10.\textless Naming Authority\textgreater & / & \textless doi name syntax\textgreater \\ \hline
  \end{tabular}}
\end{center}
\end {table}


\subsubsection{URN}
The URN PID standard was first introduced in RFC 1737 \cite{rfc1737}. It is based on the URI syntax and its syntax is shown in figure \ref{tab:pid}. The Namespace Identifier (\texttt{\textless NID\textgreater}) part identifies the namespace or to be more specific the authority that publishes the specific URN.
The syntax of the Namespace Specific String (\texttt{\textless NSS\textgreater}) part depends on the authority identified by the \texttt{\textless NID\textgreater}. This part can be used from the authority for further delegation to sub-authorities \cite{icn-bd}.

The national library of the Netherlands uses URN's and maintains a web accessed URN resolver that can resolve a URN to a URL, which can be then used to retrieve the digital object \footnote{http://resolver.kb.nl/resolve?urn=anp:1938:10:01:2:mpeg21}.
They store the metadata of a digital object in an XML scheme, which can be requested via an API call \cite{kb-urn}. The metadata can be parsed to be used in an NDN name to fill in possible naming gaps as discussed by Olschanowsky et. al. \cite{ndn-clim}. This method applies to all upcoming PID types discussed in this section.

\subsubsection{Handle}\label{hndl}
The Handle system was developed by Bob Kahn at the CNRI in 1994, and currently administered and maintained by the DONA Foundation. The CNRI is the root server of the Handle System and maintains all the Handle naming authorities, where each Handle naming authority can establish its own resolution infrastructure. This makes it possible for the GHR to delegate queries for Handle resolution. 
Its main functionalities are specified in RFC 3650, like uniqueness, which means that every handle is globally unique within the Handle System and persistence, which means that Handles may be used as persistent identifiers for internet resources \cite{rfc3650}. In 2015 it supported, on average, 68 million resolution requests per month where the largest single user being the DOI system, which will be discussed in more detail in section \ref{doi} \cite{hdl-us}. 

The Handle syntax is shown in table \ref{tab:pid}. The \texttt{\textless Handle Naming Authority\textgreater} (HNA) part of the syntax is a prefix that it is assigned by the GHR and its hierarchical structure is similar to DNS domain names. The HNA is a sequence of decimals that are separated by the dot \texttt{(".")} character. For example one can get the prefix \texttt{20.4000.581} assigned. The slash \texttt{("/")} delimiter separates the HNA from the \texttt{\textless Local Handle Name\textgreater}(LHN) syntax. In this part a PID provider can specify the identity of a digital object within the HNA it gets assigned. The only limitation for the LHN syntax is that it can only contain printable characters from Unicode's UCS-2 character set. The path of a Handle PID is read from the left to the right and the dot \texttt{(".")} character defines the hierarchy of the naming authorities. The naming hierarchy does not imply any technical implication. Which means that the HNA \texttt{"20.5000.481"} can be independent of the HNA \texttt{"20.5000"} \cite{icn-bd}. For Handles, a web accessed URN resolver can also be used to resolve a Handle PID to a URL. This is shown in the following URL for example \url{http://hdl.handle.net/20.5000.481/data/objects/object1}. Metadata is stored in JSON and the Handle System makes use of the restful JSON API for retrieving metadata \cite{hdl-api}.

\subsubsection{DOI}\label{doi}
DOI makes use of the Handle system as described in section \ref{hndl}. The Handle System was selected for resolving DOI's because it matched the resolution requirements identified for the DOI concept \cite{doi-found}, and thus DOI is based on Handle. It is managed and controlled by the IDF. The DOI system has been assigned the \texttt{<Handle Naming Authority>} value 10 in the GHS as shown in table \ref{tab:pid}. Just like Handle, it uses a slash \texttt{("/")} delimiter to separate the PID authority from the PID name.

The DOI system is mostly an administrative framework for assuring common practices and standards for publishing and maintaining handles between the RAs. An RA is an organization or institution which ensures specific quality standards in order to participate in the DOI project and is responsible for assigning DOI's to digital objects. The \texttt{<doi name syntax>} part identifies an object within the DOI naming authority. The DOI Resolver is the apex in the hierarchy and it can resolve any RA's DOI to a URL from which the digital object can be retrieved \cite{icn-bd}. This makes the resolution of a DOI to the digital object also achievable by a web accessed resolver. The PANGAEA information system, aimed at archiving, publishing and distributing georeferenced data from earth system research, uses DOI \cite{pang}. A DOI URL used by PANGAEA is for example \texttt{https://doi.org/10.1594/PANGAEA.339110}.
Furthermore, in the DOI system metadata is available in many formats, including BibTeX, Citeproc-JSON, RDF as well as XML and JSON \cite{doi-met}. It depends on the PID provider which format to use.

\subsection{NDN}
\label{overview-ndn}
The focus of NDN is to change network behavior by removing the restriction that packets can only name communication endpoints \cite{ndn-summary}. In an NDN network the endpoint is a named chunk of a video, book or data set, which can be forwarded by name. NDNâ€™s minimal functionality includes support for consumer-driven data delivery, built-in data security, and use of in-network caching. NDN provides support for scaling data, balancing data flows for congestion control and retrieving data via multiple paths. This is done by routing packets based on their name rather than destination. When data is sent across the NDN network, it's cached at intermediary hops. Consecutive request for the same data can then be provided by these local in-network caches.

In NDN the content can be retrieved from any source that has the named data. Data is named in NDN by typically a hierarchical name, in which a prefix can be used to match a specific tree of an organization. \todo{Find better illustration for NDN name} Figure \ref{fig:ndn_name}, taken from Jacobson et al. \cite{jacobson2009networking} is an example of an NDN named video file. This named data object is made available under the prefix \texttt{/parc.com}, which is the globally-routable name. This globally-routable name is in practice assigned by an organization (operations similar to DNS allocation of today). All data under this prefix are automatically published and available in NDN. These can then be downloaded from the original publisher, a repository, a router cache or a neighboring peer in a local network. NDN can be used as an overlay on any type of network e.g. TCP/IP, but also Bluetooth. All content can be authenticated by the use of integrity checks to ensure untainted copies of the data. Efficient distribution is achieved by caching at intermediary hops in the NDN network. These hops can be NDN routers, but also cellphones and laptops. This distributed nature of NDN provides parallel transfers such as bulk data distribution and thus collaboration simultaneously on the same datasets, such as climate change data or a movie.

\begin{figure}[H]
\centering
\includegraphics[width=\columnwidth/2]{Images/ndn_name.png}
\caption{NDN name scheme example.}
\label{fig:ndn_name}
\end{figure}

The receiving end in NDN is in control of communication. Two distinct packets are used to drive communication; interest and data packets. In order to query for data names in the NDN network, the interest packet is used. When this interest packet is received by a node in the NDN network that has the data, a data packet is returned. This data packets is send back over the same route as the interest packet was sent, resulting in symmetric forwarding. As discussed earlier, data may be cached on the intermediary hops in the NDN network.

NDN has an advantages over TCP/IP, it's able to use multiple paths and unlike TCP/IP it's able to handle loops at the forwarding layer. A loop-free topology is realized by inserting a nonce in every interest packet which allows to identify duplicate interests for named data, allowing multiple paths to be used which could increase throughput efficiency. Information about paths in the network are maintained in a layer called the strategy layer. This layer keeps track of two-way traffic and changes local forwarding decisions based on traffic observations. A face in NDN holds a connection to a forwarder, which could be of any class of the underlay (e.g. TCP/IP, Bluetooth or any other supported underlay).

The NDN packet forwarding engine is composed out of three data structures; The FIB, CS and PIT. The FIB is used to forward interest packets towards potential source(s) of matching data. The FIB allows a list of outgoing faces towards a certain prefixes. In contrast to an IP FIB, a list for a single destination is not allowed due to network loop problems. The behavior of the forwarding can be changed by using different forwarding strategies. Before an interest packet is send out to the FIB it is first stored in the PIT along with the requesting face. An interest packet will remain in the PIT until a user defined timeout is reached, or when a matching data packet has returned from a source. The prefix and requesting face in the PIT define a return path back to the requester at each hop, this is referred to as the breadcrumbs path. The CS acts as the cache in NDN, which is used to provide the distributed property in NDN. This cache also has strategies, one for evicting data objects from the cache and one for stashing data objects in the cache.

\subsubsection{NDN strategies}
As described in section \ref{overview-ndn} there are two kinds of caching strategies; a caching decision and a cache replacement strategy. A caching decision strategy is used to determine in which router along the reverse path of an interest packet will cache the data \cite{koulouzis2018information}. Below are the most well-known caching decision strategies are listed.

\begin{itemize}
    \item Leave copy everywhere; used to cache data object packets along each hop in the NDN path.
    \item Leave copy down; used only by the first router in the NDN path after either the original producer or an NDN cache.
    \item Leaving copies with probability; used to cache those data objects with the probability of 1/(hop count).
    \item Leaving copies with uniform probability; used to cache data objects with uniform probability.
\end{itemize}

A cache replacement strategy is used to determine which data objects to evict from the cache in order to make room for new data objects. Below are the most well-known cache eviction strategies are listed.
\begin{itemize}
    \item First in first out; used to evict the first data object that was inserted from the cache.
    \item Random replacement; used to evict random data objects from the cache.
    \item Least recently used; used to evict the least recently used data object from the cache.
    \item Least frequently used; used to evict the least frequently used data object from the cache.
\end{itemize}

The information stored in the PIT and FIB can be used to determine how to forward an interest packet out to one or more faces. These strategies are meant to give adaptive decisions based on network conditions. Below are the most well-known forward strategies listed.
\begin{itemize}
    \item Flooding; used to forward received interest packets towards every face, excluding the face the interest was received from.
    \item Best-route with caching; used to calculate the best path with Dijkstra's algorithm (least amount of hops) to reach a data object cache.
    \item Best-route without caching; used to only satisfy interest packets towards the original data publishers, thus excluding intermediary caches along the NDN path.
\end{itemize}

\subsection{McCabe}
\label{overview-mccabe}
As briefly described in section \ref{introduction-related-work}, McCabe's book "Network Analysis, Architecture, and Design" \cite{mccabe2010network} is about applying a systems methodology approach towards network design. McCabe's approach consists out of three core phases; analysis, architecture and design. These phases in McCabe describe how to make technology and topology decisions in a network. These decisions are guided based on inputs for the three core phases, the initial input can be from users and/or from network metrics. Consecutive processes use the output of previous processes as input, thus these processes are interconnected. In this section we will describe McCabe's method in more detail.

In figure \ref{fig:mccabe-process} the three core phases are illustrated in \textit{italic} while the processes are in normal text. In order to highlight the core phases by name they are \underline{underlined} in the figure. The first phase (analysis) has as goal to understand the network and potential problems in terms of performance and efficiency in order to determine network requirements. This is done by developing sets of problem statements and objectives that describe what the target network should address. Therefore, historical data from network management (monitoring), requirements gathered from the network users, staff and management are included in the analysis phase. Furthermore, these metrics are then compared to the relationships between users, applications, devices and other networks in order to determine if requirements match with the user and network expectations. The second and third phase (architecture and design) uses the output of the first phase to establish a high-level design of the network. This network design determines which technology and topology choices are justified to improve the network requirements established in the first phase. The fourth process is implementing the design, test if requirements are met and finally accept the implementation. These phases are intended to be iterative and by no means define a final architecture design. This is due to the fact that requirements, technology and user behaviour can change and with that the network design.

\begin{figure}[H]
\centering
\includegraphics[width=\columnwidth]{Images/mccabe-process.png}
\caption{Cyclic and iterative nature of McCabe's processes and phases.}
\label{fig:mccabe-process}
\end{figure}

\subsection{TOSCA}
\label{overview-tosca}
% Make sure you talk about templates, not just descriptions
% Describe DRIP, OpenStack and an orchestrator in general
OASIS, an organization which is a global nonprofit consortium that works on open-standards, published the first TOSCA standard in 2014. The TOSCA standard is meant to standardize data modeling for cloud orchestration environments and applications. The latest version 1.1 standard \cite{tosca-standard}, was released in 2018. In this section we will highlight the important features of TOSCA, relevant to our research scope. Section \ref{planning-deploying} will merge TOSCA with McCabe's method (section \ref{overview-mccabe}).

Portability and automated management of enterprise IT infrastructures are major concerns. Infrastructures may need to run on heterogeneous components due to application needs. Having different descriptions of deployment and management for each environment complicates an infrastructure. This challenge introduces new requirements and concepts for deployment, configuration, operation and termination of components that make up an entire infrastructure. TOSCA provides a method to describe these components in generic modeling templates. In which an orchestrator can be used to implement these descriptions. Thus, TOSCA provides a single template description for an entire infrastructure, which is then implemented by an orchestrator. Several TOSCA orchestrators are e.g. DRIP and OpenStack. DRIP is currently a prototype which uses the open cloud computing interface and currently supports Amazon EC2\footnote{\url{https://aws.amazon.com/ec2/}}, EGI FedCloud\footnote{\url{https://www.egi.eu/services/cloud-compute/}} and ExoGeni\footnote{\url{http://www.exogeni.net/}} clouds. Furthermore, DRIP also supports Ansible playbooks\footnote{\url{https://www.ansible.com/}} for configuration management and contains a deployment agent for e.g. Kubernetes (discussed in section \ref{overview-virtualization}). This results into a portable (can be run by any orchestrator that understands TOSCA) and automated (implementation carried out by an orchestrator) method of infrastructure management. This allows interoperability and reusability of TOSCA template descriptions on different cloud providers. Seaclouds\footnote{\url{http://www.seaclouds-project.eu/}} (EU FP7 funded project), not to be confused with SeaDataCloud, is already managing infrastructures based on TOSCA. Google, Red Hat, Canonical and IBM are also involved with the development of TOSCA, signifying that broader adoption may follow in the future.

TOSCA template descriptions consist out of the following core components; nodes, relationships and interfaces. Nodes can be a host, container or VM and are connected to each other through relationships such as 'dependsOn', 'hostedOn' and 'connectsTo'. These relationships can be used to describe that a VM is 'hosted on' a host (e.g. a bare metal machine). Or that a set of containers 'depend on' each other for functionality and 'connect to' e.g. a database. Such as a containerized web application that requires a database, facilitated by another container. Interfaces are used to control the life cycle of a component and consist as a set of hooks to trigger actions, these actions are create, configure, start, stop or delete. These hooks can be triggered to e.g. configure and create containers, stop or start a service or do system maintenance such as delete artifacts after a service is stopped. Furthermore, constraints can be set for the input values in these template descriptions. These can be e.g. that the amount of CPU's should be defined with integers and be should contain a value more than one. However, the orchestrator is responsible for verifying these constraints, TOSCA is merely a means to describe components in an infrastructure. 

\subsection{Virtualization}
\label{overview-virtualization}
Virtualization is part of modern day infrastructures. It provides more efficient use of resources and flexibility because multiple VMs and containers can be deployed on any host that support them. VMs are virtual operating systems that run in complete isolation from its host. Containers on the other hand share the kernel of its host and offer user-space isolation, which is relatively less isolation when compared to a VM. However, containers are more resource friendly than VMs in terms of needed compute power, memory and disk space. This section will briefly describe these two forms of virtualization.

Cloud providers such as Amazon provide easy access to compute resources, one way to access these resources is by deploying a VM on their platform. Resources can be added or removed via management interfaces or API's. This allows an infrastructure, that e.g. runs on the Amazon cloud platform, to flexibly scale in or out. Containers offer the ability to package software components together with all dependencies (e.g. libraries and binaries), such a package is called a container image. Docker is a popular platform to build and manage container-based virtualization. Once a Docker image is built it can be distributed through so-called Docker registries, such as Docker Hub\footnote{\url{https://hub.docker.com/}}. An important property of containers is that they aren't persistent, this means that containers revert back to their original image state and that data created inside the container is lost when the container has stopped or crashed. In order to make containers persistent, a data volume can be mapped onto the container host, thus storing data outside the container. Kubernetes\footnote{\url{https://kubernetes.io/}} can be used to automate application deployment, scaling, and management of containers. In Kubernetes a container is called a pod, this may be a Docker container. These Kubernetes pods can be deployed in a cluster of Kubernetes nodes, spanning over e.g. different data centers. This offers great flexibility by packaging an application in a pod and then scale the application in or out through different data centers with Kubernetes. Kubernetes also offers high-availability features, such as automatically restart a pod when a crash occurs or load-balance requests by using a pool of identical pods in e.g. a round-robin fashion. This can be done by configuring a logical set of pods as a service. In order to custom tailor pods, environment variables can be used. These environment variables are then available inside the pod's namespace. Scripts that are executed inside the pod can then use these environment variables to setup e.g. a network by specifying routes and a gateway. In Docker these scripts can be executed via a so called \texttt{ENTRYPOINT}\footnote{\url{https://docs.docker.com/engine/reference/builder/\#entrypoint}}.

NFV is a network architecture concept which uses virtualization to create and manage higher level network functions, as software, on commodity hardware. These functions may be interconnected to create a network service such as load balancers, firewalls or intrusion detection. This architecture differs from traditional network architectures, where network functions are provided by hardware devices. NFV provides the ability to easily duplicate network functionality and expand the network locally or into other data centers. NFV is usually managed by an orchestrator to automate deployment, thus providing less overhead than traditional network management with hardware devices.